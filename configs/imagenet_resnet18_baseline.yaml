#===============================================================================
#            Default Configuration for baseline ResNet18 on ImageNet
#===============================================================================
# This configuration file is for training and evaluating a baseline ResNet18 on 
# the ImageNet dataset.
# It includes settings for environment, model architecture, quantization parameters
#============================ Environment ======================================

# Experiment name
project:
  name: imagenet_resnet18_baseline
  run_name: ResNet18_Baseline
  # -1 is Random seed, any other positive Number is static seed for reproducibility
  seed: -1
  # Name of output directory. Checkpoints and logs will be saved at `pwd`/output_dir
  output_dir: output

# Device to be used
device:
  # Use CPU or GPU (choices: cpu, cuda)
  type: cuda
  # GPU device IDs to be used. Only valid when device.type is 'cuda'
  gpu: [0, 1, 2, 3, 4, 5]

# Dataset loader
dataloader:
  # Dataset to train/validate (choices: imagenet)
  dataset: imagenet
  # Number of categories in the specified dataset (choices: 1000)
  num_classes: 1000
  image_size: 224
  channels: 3
  # Path to dataset directory
  path: /home/ehoffer/Datasets/imagenet
  # Size of mini-batch
  batch_size: 256
  # Number of data loading workers
  workers: 32
  # Seeds random generators in a deterministic way (i.e., set all the seeds 0).
  # Please keep it true when resuming the experiment from a checkpoint
  deterministic: true
  # Load the model without DataParallel wrapping it
  serialized: false
  # Portion of training dataset to set aside for validation (range: [0, 1))
  val_split: 0.05
  augmentation:
    # Random resized crop size for training images
    resize_crop: 224
    # Random horizontal flip for training images
    hflip: true
    # Color jittering for training images
    color_jitter: false
    # Auto augmentation policy (choices: imagenet, cifar10, svhn, none)
    auto_augment: none
  normalize:
    # Mean for image normalization
    mean: [0.485, 0.456, 0.406]
    # Std for image normalization
    std: [0.229, 0.224, 0.225]


resume:
  # Path to a checkpoint to be loaded. Leave blank to skip
  path:
  # Resume model parameters only
  lean: false

log:
  # Number of best scores to track and report
  num_best_scores: 3
  # Print frequency
  print_freq: 20

#============================ Model ============================================

# Supported model architecture
# choices:
#   ImageNet:
#     resnet18, resnet34, resnet50, resnet101, resnet152
arch: resnet18

# Use pre-trained model
pre_trained: false

#============================ Quantization =====================================

quan:
  act: # (default for all layers)
    # Quantizer type (choices: lsq)
    mode: lsq
    # Bit width of quantized activation
    bit: 3
    # Each output channel uses its own scaling factor
    per_channel: false
    # Whether to use symmetric quantization
    symmetric: false
    # Quantize all the numbers to non-negative
    all_positive: true
  weight: # (default for all layers)
    # Quantizer type (choices: lsq)
    mode: lsq
    # Bit width of quantized weight
    bit: 3
    # Each output channel uses its own scaling factor
    per_channel: true
    # Whether to use symmetric quantization
    symmetric: false
    # Whether to quantize all the numbers to non-negative
    all_positive: false
  excepts:
    # Specify quantized bit width for some layers, like this:
    conv1:
      act:
        all_positive: false
      weight:
        bit:
    fc:
      act:
        bit:
      weight:
        bit:

#============================ Training / Evaluation ============================

# Evaluate the model without training
# If this field is true, all the bellowing options will be ignored
eval: false

epochs: 2 #TODO:

optimizer:
  learning_rate: 0.1
  momentum: 0.9
  weight_decay: 0.0001

# Learning rate scheduler
lr_scheduler:
  # Update learning rate per batch or epoch
  update_per_batch: true

  # Uncomment one of bellowing options to activate a learning rate scheduling

  # Fixed learning rate
  mode: fixed

  # Step decay
  # mode: step
  # step_size: 30
  # gamma: 0.1

  # Multi-step decay
  # mode: multi_step
  # milestones: [30, ]
  # gamma: 0.1

  # Exponential decay
  # mode: exp
  # gamma: 0.95

  # Cosine annealing
  # mode: cos
  # lr_min: 0
  # cycle: 0.95

  # Cosine annealing with warm restarts
  # mode: cos_warm_restarts
  # lr_min: 0
  # cycle: 5
  # cycle_scale: 2
  # amp_scale: 0.5